# Copyright (c) 2025 MikeT LLC. All rights reserved.

---
# Hardware and OS profile for the wintermute workstation
# Desktop workstation with high-end GPU for development and gaming
hardware:
  vendor: "Desktop"
  model: "Custom Build"
  cpu:
    model: "Intel Core i9"  # TODO: Update with actual CPU model
    cores: 16  # TODO: Update with actual core count
    threads: 24  # TODO: Update with actual thread count
  gpu:
    vendor: "NVIDIA"
    model: "GeForce RTX 4070 Super"
    vram_gb: 12
    driver_version: "latest"
    cuda_capable: true
  memory_gb: 32  # TODO: Update with actual memory amount
  storage:
    - type: "NVMe SSD"
      size_gb: 1000  # TODO: Update with actual storage size
network:
  interfaces:
    - name: "Ethernet"
      type: "ethernet"
      mac_address: "{{ vault_wintermute_eth_mac | default('REDACTED') }}"
    - name: "Wi-Fi"
      type: "wifi"
      mac_address: "{{ vault_wintermute_wifi_mac | default('REDACTED') }}"
  tailscale:
    enabled: true
    hostname: "wintermute"
os:
  name: "Windows"
  version: "11"
  edition: "Pro"
  build: "22H2"
device_roles:
  - windows-workstation
  - gaming-mode
  - cuda-development
  - podman-desktop

# Remote desktop configuration
# NoMachine is the standardized remote access protocol (RDP retired 2025-11-22)
remote_protocol: nomachine
remote_port: 4000
restrict_to_tailscale: true

# NoMachine settings
nomachine_version: "9.2.18"
nomachine_port: 4000

# vLLM Configuration
vllm:
  enabled: true
  model: "casperhansen/llama-3-8b-instruct-awq"
  served_model_name: "llama31-8b-wintermute"
  port: 8000
  container_name: "vllm-wintermute"
  image: "vllm/vllm-openai:latest"
  max_model_len: 9000
  gpu_memory_utilization: 0.88
  quantization: "awq"
  tensor_parallel_size: 1
  auto_switch: true
  check_interval_minutes: 5
  idle_threshold_minutes: 5

# AI Fabric Role Assignment (aligns with miket-infra platform contract)
# This node serves: chat-deep (reasoner with longer context)
ai_node_roles:
  - chat-deep

# ========================================
# Ollama Configuration (PHC AI Fabric)
# ========================================
# NOTE: Ollama must be manually installed by user
# Install from: https://ollama.com/download/windows
# Requires: NVIDIA Studio drivers (not Game Ready)
ollama_enabled: true
ollama_models:
  - name: llama3:8b      # Base model to pull
    context: 8192
  - name: llama3-8k      # Custom model with explicit context
    context: 8192
    custom: true         # Created from llama3:8b with PARAMETER num_ctx 8192

# Note: Windows hosts don't use become/sudo
# If Wintermute were Linux, these settings would enable passwordless sudo via 1Password
# ansible_become: true
# ansible_become_method: sudo
# ansible_become_password: "{{ lookup('env', 'ANSIBLE_BECOME_PASS') | default(lookup('community.general.onepassword', 'op://Automation/wintermute/sudo', errors='ignore') | default(lookup('pipe', 'op read op://Automation/wintermute/sudo 2>/dev/null || echo'), true), true) | default('', true) }}"

# ========================================
# Netdata Monitoring Configuration
# ========================================
# wintermute is a Netdata CHILD node
# Streams metrics to motoko parent
netdata_role: child

# Tailscale IP will be auto-detected during deployment
# netdata_tailscale_ip: 100.x.x.x

# Cloud connectivity DISABLED (self-hosted only)
netdata_cloud_enabled: false
