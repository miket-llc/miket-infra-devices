---
# LiteLLM Proxy Configuration for Motoko
# Override secrets at runtime or via Ansible Vault

# LiteLLM version (pinned for stability)
litellm_version: "latest"

# Network binding
litellm_bind_host: "0.0.0.0"
litellm_port: 8000

# Backend service URLs (using local network IPs - Tailscale routing has issues)
armitage_base_url: "http://192.168.1.157:8000/v1"
wintermute_base_url: "http://192.168.1.93:8000/v1"
motoko_reasoning_base_url: "http://motoko.pangolin-vega.ts.net:8001/v1"
motoko_embed_base_url: "http://motoko.pangolin-vega.ts.net:8200/v1"

# Wintermute reasoner model (defaults to Llama 3.1 8B AWQ). To switch to Gemma,
# change these three vars only.
wintermute_model_name: "local/reasoner"
wintermute_model_display: openai/casperhansen/llama-3-8b-instruct-awq
wintermute_model_hf_id: "casperhansen/llama-3-8b-instruct-awq"

# --- Gemma option (uncomment to use) ---
# wintermute_model_display: "openai/gemma-2-9b-it-awq"
# wintermute_model_hf_id: "google/gemma-2-9b-it-AWQ"

# OpenAI configuration
openai_strong_model: "gpt-4.1-mini"
openai_cheap_model: "gpt-4o-mini"
openai_budget_monthly_usd: 150

# Secrets loaded from vault (ansible/group_vars/linux/vault.yml)
# These reference vault variables which are encrypted
openai_api_key: "{{ vault_openai_api_key | default('sk-REPLACE_ME') }}"
litellm_bearer_token: "{{ vault_litellm_bearer_token | default('mkt-REPLACE_ME_SUPERLONG') }}"

