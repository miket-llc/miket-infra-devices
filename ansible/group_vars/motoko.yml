---
# LiteLLM Proxy Configuration for Motoko
# Override secrets at runtime or via Ansible Vault

# LiteLLM version (pinned for stability)
litellm_version: "latest"

# Network binding
litellm_bind_host: "0.0.0.0"
litellm_port: 8000

# Backend service URLs (using local network IPs - Tailscale routing has issues)
armitage_base_url: "http://192.168.1.157:8000/v1"
wintermute_base_url: "http://192.168.1.93:8000/v1"
motoko_reasoning_base_url: "http://motoko.pangolin-vega.ts.net:8001/v1"
motoko_embed_base_url: "http://motoko.pangolin-vega.ts.net:8200/v1"

# Wintermute reasoner model (defaults to Llama 3.1 8B AWQ). To switch to Gemma,
# change these three vars only.
wintermute_model_name: "local/reasoner"
wintermute_model_display: openai/llama31-8b-wintermute
wintermute_model_hf_id: "casperhansen/llama-3-8b-instruct-awq"

# Wintermute model capabilities (must match ansible/host_vars/wintermute.yml)
wintermute_max_model_len: 9000
wintermute_max_input_tokens: 8000  # 85% of max_model_len for safety
wintermute_max_output_tokens: 1000
wintermute_max_concurrent_requests: 2

# Armitage model capabilities (must match ansible/host_vars/armitage.yml)
armitage_model_name: "qwen2.5-7b-armitage"
armitage_max_model_len: 8192
armitage_max_input_tokens: 7000  # 85% of max_model_len for safety
armitage_max_output_tokens: 768
armitage_max_concurrent_requests: 1

# --- Gemma option (uncomment to use) ---
# wintermute_model_display: "openai/gemma-2-9b-it-awq"
# wintermute_model_hf_id: "google/gemma-2-9b-it-AWQ"

# OpenAI configuration
openai_strong_model: "gpt-4.1-mini"
openai_cheap_model: "gpt-4o-mini"
openai_budget_monthly_usd: 150

# Secrets supplied via env files generated from Azure Key Vault
openai_api_key: "{{ lookup('env', 'OPENAI_API_KEY') | default('__SECRET_OPENAI_API_KEY__', true) }}"
litellm_bearer_token: "{{ lookup('env', 'LITELLM_TOKEN') | default('__SECRET_LITELLM_TOKEN__', true) }}"
