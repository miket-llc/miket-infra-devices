# Copyright (c) 2025 MikeT LLC. All rights reserved.

---
# =============================================================================
# MOTOKO CONTAINERS PLAYBOOK
# =============================================================================
#
# Complete container stack deployment for motoko
# Provisions storage, configures runtime, and deploys all services
#
# USAGE:
#   Full deployment:
#     ansible-playbook -i inventory/hosts.yml playbooks/motoko/containers.yml
#
#   Specific components (via tags):
#     ansible-playbook ... --tags storage      # Second drive + mounts
#     ansible-playbook ... --tags runtime      # Podman configuration
#     ansible-playbook ... --tags vllm         # vLLM services only
#     ansible-playbook ... --tags litellm      # LiteLLM proxy only
#     ansible-playbook ... --tags nextcloud    # Nextcloud stack only
#
#   Dry run:
#     ansible-playbook ... --check
#
# PREREQUISITES:
#   - Base system configured (site.yml already run)
#   - NVIDIA drivers installed and working
#   - Tailscale connected
#
# STORAGE LAYOUT (on second NVMe - /space):
#   /space/containers/engine/podman  - Podman graphroot
#   /space/apps/vllm                 - vLLM models and cache
#   /space/apps/litellm              - LiteLLM configuration
#   /space/apps/nextcloud            - Nextcloud data
#   /space/services                  - Additional service configs
#
# =============================================================================

- name: Deploy Container Stack on Motoko
  hosts: motoko
  become: true
  gather_facts: true

  vars:
    ansible_python_interpreter: /usr/bin/python3

  pre_tasks:
    - name: Display deployment banner
      ansible.builtin.debug:
        msg: |
          ═══════════════════════════════════════════════════════════════
          MOTOKO CONTAINER STACK DEPLOYMENT
          ═══════════════════════════════════════════════════════════════
          
          Host:           {{ inventory_hostname }}
          Distribution:   {{ ansible_distribution }} {{ ansible_distribution_version }}
          
          Deployment Phases:
            1. data_mounts_psbi   - Mount /space on second NVMe
            2. podman_base        - Podman runtime with NVIDIA + custom storage
            3. vllm-motoko        - vLLM reasoning + embeddings services
            4. litellm_proxy      - LiteLLM API gateway
            5. nextcloud_server   - Nextcloud file sync (if enabled)
          
          Storage Target: /space (UUID: {{ (psbi_mounts | selectattr('path', 'equalto', '/space') | first).uuid | default('not set') }})
          
          ═══════════════════════════════════════════════════════════════
      tags: [always]

    - name: Verify NVIDIA drivers are working
      ansible.builtin.command: nvidia-smi --query-gpu=name --format=csv,noheader
      register: nvidia_check
      changed_when: false
      failed_when: false
      tags: [always]

    - name: Display GPU status
      ansible.builtin.debug:
        msg: "GPU: {{ nvidia_check.stdout | default('Not detected - GPU containers will fail') }}"
      tags: [always]

  roles:
    # ========================================
    # Phase 1: Storage - Mount Second Drive
    # ========================================
    # Mounts /space on the second NVMe (nvme1n1p3)
    # Creates directory structure for container data
    - role: data_mounts_psbi
      tags: [storage, mounts]

    # ========================================
    # Phase 2: Container Runtime
    # ========================================
    # Installs Podman with:
    # - Custom graphroot on /space
    # - NVIDIA Container Toolkit
    # - Docker CLI compatibility
    - role: podman_base
      tags: [runtime, podman]

    # ========================================
    # Phase 3: vLLM AI Services
    # ========================================
    # Deploys:
    # - Reasoning model (Mistral 7B AWQ) on port 8001
    # - Embeddings model (BGE Base) on port 8200
    - role: vllm-motoko
      when: vllm_enabled | default(true)
      tags: [vllm, ai, llm]

    # ========================================
    # Phase 4: LiteLLM Proxy
    # ========================================
    # API gateway for LLM endpoints
    # Routes to local vLLM + remote models
    - role: litellm_proxy
      when: litellm_enabled | default(true)
      tags: [litellm, ai, proxy]

    # ========================================
    # Phase 5: Nextcloud (Optional)
    # ========================================
    # Self-hosted file sync and share
    # Requires secrets from Azure Key Vault
    - role: nextcloud_server
      when: nextcloud_enabled | default(false)
      tags: [nextcloud, storage]

  post_tasks:
    # ========================================
    # Create App Directories
    # ========================================
    - name: Ensure app directories exist on /space
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /space/apps
        - /space/apps/vllm
        - /space/apps/vllm/models
        - /space/apps/vllm/cache
        - /space/apps/litellm
        - /space/containers
        - /space/containers/engine
        - /space/containers/engine/podman
        - /space/services
      when: ansible_mounts | selectattr('mount', 'equalto', '/space') | list | length > 0
      tags: [storage, setup]

    # ========================================
    # Verification
    # ========================================
    - name: Check container runtime
      ansible.builtin.command: podman info --format json
      register: podman_info
      changed_when: false
      failed_when: false
      tags: [verify]

    - name: Check running containers
      ansible.builtin.command: podman ps --format "table {% raw %}{{.Names}}\t{{.Status}}\t{{.Ports}}{% endraw %}"
      register: running_containers
      changed_when: false
      failed_when: false
      tags: [verify]

    - name: Display deployment summary
      ansible.builtin.debug:
        msg: |
          ═══════════════════════════════════════════════════════════════
          MOTOKO CONTAINER STACK DEPLOYMENT COMPLETE
          ═══════════════════════════════════════════════════════════════
          
          ✅ Storage
             Mount: /space
             Podman graphroot: {{ podman_graphroot | default('/space/containers/engine/podman') }}
          
          ✅ Container Runtime
             Podman: {{ (podman_info.stdout | default('{}') | from_json).version.Version | default('installed') }}
             NVIDIA: {{ 'Configured' if (podman_nvidia_enabled | default(true)) else 'Disabled' }}
          
          ✅ AI Services
             vLLM Reasoning:  http://127.0.0.1:{{ vllm_reasoning_port | default(8001) }}/v1
             vLLM Embeddings: http://127.0.0.1:{{ vllm_embeddings_port | default(8200) }}/v1
             LiteLLM Proxy:   http://127.0.0.1:{{ litellm_port | default(8000) }}/v1
          
          ✅ Running Containers
          {{ running_containers.stdout | default('No containers running yet') }}
          
          ═══════════════════════════════════════════════════════════════
          
          NEXT STEPS:
          
          1. Verify services are starting (may take a few minutes for model download):
             podman ps
             podman logs vllm-reasoning-motoko
          
          2. Test vLLM health:
             curl http://127.0.0.1:8001/health
             curl http://127.0.0.1:8200/health
          
          3. Test LiteLLM:
             curl http://127.0.0.1:8000/v1/models
          
          4. Check GPU utilization:
             nvidia-smi
          
          ═══════════════════════════════════════════════════════════════
      tags: [always]

