# Copyright (c) 2025 MikeT LLC. All rights reserved.

---
# Unified vLLM deployment playbook for Windows workstations
# Deploys vLLM configuration, scripts, and Docker containers to wintermute and armitage
# Usage:
#   ansible-playbook -i inventory/hosts.yml playbooks/windows-vllm-deploy.yml --limit wintermute --ask-vault-pass
#   ansible-playbook -i inventory/hosts.yml playbooks/windows-vllm-deploy.yml --limit armitage --ask-vault-pass
#   ansible-playbook -i inventory/hosts.yml playbooks/windows-vllm-deploy.yml --limit windows_workstations --ask-vault-pass

- name: Deploy vLLM to Windows workstations
  hosts: windows_workstations
  gather_facts: false
  vars:
    # PowerShell scripts to deploy (device-specific)
    vllm_powershell_scripts:
      - src: "{{ playbook_dir }}/../../devices/{{ inventory_hostname }}/scripts/Start-VLLM.ps1"
        dest: "{{ vllm_defaults.scripts_path }}\\Start-VLLM.ps1"
      - src: "{{ playbook_dir }}/../../devices/{{ inventory_hostname }}/scripts/Set-WorkstationMode.ps1"
        dest: "{{ vllm_defaults.scripts_path }}\\Set-WorkstationMode.ps1"
    # Include WSL2 bash script for wintermute (deployed conditionally in role)
    vllm_bash_scripts:
      - src: "{{ playbook_dir }}/../../devices/wintermute/scripts/vllm.sh"
        dest: "{{ vllm_defaults.scripts_path }}\\vllm.sh"
  
  roles:
    - windows-vllm-deploy
  
  post_tasks:
    - name: Display deployment summary
      debug:
        msg:
          - ""
          - "âœ… vLLM deployment complete for {{ inventory_hostname }}"
          - ""
          - "Configuration:"
          - "  Model: {{ vllm.model }}"
          - "  Served Model Name: {{ vllm.served_model_name | default('N/A') }}"
          - "  Max Model Length: {{ vllm.max_model_len }}"
          - "  GPU Memory Utilization: {{ vllm.gpu_memory_utilization }}"
          - "  Port: {{ vllm.port }}"
          - ""
          - "Scripts deployed to: {{ vllm_defaults.scripts_path }}"
          - "Config deployed to: {{ vllm_defaults.config_path }}"
          - ""
          - "To start vLLM manually:"
          - "  powershell -File {{ vllm_defaults.scripts_path }}\\Start-VLLM.ps1 -Action Start"
          - ""
          - "API will be available at:"
          - "  http://{{ inventory_hostname }}.pangolin-vega.ts.net:{{ vllm.port }}/v1"
          - ""
