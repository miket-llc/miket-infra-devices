# {{ ansible_managed }}
# LiteLLM Proxy Configuration
# Routes requests to vLLM backends on the tailnet
# See: docs/runbooks/LITELLM_PROXY.md

model_list:
{% for backend in vllm_backends %}
{% for model in backend.models %}
  # ============================================================================
  # {{ model.alias }} â†’ {{ backend.host }}:{{ backend.port }}
  # ============================================================================
  - model_name: {{ model.alias }}
    litellm_params:
      model: openai/{{ model.id }}
      api_base: "http://{{ backend.host }}:{{ backend.port }}/v1"
      api_key: "not-needed"  # vLLM doesn't require API key
{% if model.max_model_len is defined %}
    model_info:
      mode: chat
      max_input_tokens: {{ model.max_input_tokens | default(model.max_model_len - 2048) }}
      max_output_tokens: {{ model.max_output_tokens | default(2048) }}
      max_tokens: {{ model.max_model_len }}
{% endif %}
{% if model.tpm_limit is defined %}
    tpm_limit: {{ model.tpm_limit }}
{% endif %}
{% if model.rpm_limit is defined %}
    rpm_limit: {{ model.rpm_limit }}
{% endif %}
{% if model.max_concurrent_requests is defined %}
    max_concurrent_requests: {{ model.max_concurrent_requests }}
{% endif %}

{% endfor %}
{% endfor %}
  # ============================================================================
  # DEFAULT MODEL ALIAS
  # ============================================================================
  # Routes "default" to the primary model for clients that don't specify
  - model_name: default
    litellm_params:
      model: openai/{{ vllm_backends[0].models[0].id }}
      api_base: "http://{{ vllm_backends[0].host }}:{{ vllm_backends[0].port }}/v1"
      api_key: "not-needed"
{% if vllm_backends[0].models[0].max_model_len is defined %}
    model_info:
      mode: chat
      max_input_tokens: {{ vllm_backends[0].models[0].max_input_tokens | default(vllm_backends[0].models[0].max_model_len - 2048) }}
      max_output_tokens: {{ vllm_backends[0].models[0].max_output_tokens | default(2048) }}
      max_tokens: {{ vllm_backends[0].models[0].max_model_len }}
{% endif %}

{% if litellm_openai_fallback_enabled | default(false) %}
  # ============================================================================
  # UPSTREAM FALLBACK PROVIDERS (Optional)
  # ============================================================================
  - model_name: openai/strong
    litellm_params:
      model: "{{ openai_strong_model }}"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      mode: chat

  - model_name: openai/cheap
    litellm_params:
      model: "{{ openai_cheap_model }}"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      mode: chat
{% endif %}

# ============================================================================
# LITELLM SETTINGS
# ============================================================================
general_settings:
  database_url: null  # Stateless operation - no database
  # master_key disabled - tailnet provides access control

# Router settings for graceful fallback
router_settings:
  routing_strategy: "simple-shuffle"  # Round-robin between healthy backends
  num_retries: 2
  timeout: 120
  retry_after: 5
  allowed_fails: 3  # Mark backend unhealthy after 3 consecutive failures
