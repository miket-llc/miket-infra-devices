# {{ ansible_managed }}
# LiteLLM Proxy Configuration
# Routes requests to logical AI Fabric roles (from miket-infra platform contract)
# Physical backends: Armitage (chat-fast), Wintermute (chat-deep), Motoko (embeddings-general)

model_list:
  # ============================================================================
  # LOGICAL ROLE: chat-fast
  # ============================================================================
  # Fast, low-latency chat for default use cases
  # Backend: Armitage (Qwen2.5-7B-AWQ, 8k context, RTX 4070 8GB)
  - model_name: chat-fast
    litellm_params:
      model: openai/{{ armitage_model_name }}
      api_base: "{{ armitage_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ armitage_max_input_tokens }}
      max_output_tokens: {{ armitage_max_output_tokens }}
      max_tokens: {{ armitage_max_model_len }}
    tpm_limit: 80000
    rpm_limit: 40
    max_concurrent_requests: {{ armitage_max_concurrent_requests }}

  # Legacy alias for backwards compatibility
  - model_name: local/chat
    litellm_params:
      model: openai/{{ armitage_model_name }}
      api_base: "{{ armitage_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ armitage_max_input_tokens }}
      max_output_tokens: {{ armitage_max_output_tokens }}
      max_tokens: {{ armitage_max_model_len }}
    tpm_limit: 80000
    rpm_limit: 40
    max_concurrent_requests: {{ armitage_max_concurrent_requests }}

  # ============================================================================
  # LOGICAL ROLE: chat-deep
  # ============================================================================
  # Deeper reasoning with longer context
  # Backend: Wintermute (Llama-3-8B-AWQ, 9k context, RTX 4070 Super 12GB)
  - model_name: chat-deep
    litellm_params:
      model: {{ wintermute_model_display }}
      api_base: "{{ wintermute_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ wintermute_max_input_tokens }}
      max_output_tokens: {{ wintermute_max_output_tokens }}
      max_tokens: {{ wintermute_max_model_len }}
    tpm_limit: 80000
    rpm_limit: 40
    max_concurrent_requests: {{ wintermute_max_concurrent_requests }}

  # Legacy alias for backwards compatibility
  - model_name: {{ wintermute_model_name }}
    litellm_params:
      model: {{ wintermute_model_display }}
      api_base: "{{ wintermute_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ wintermute_max_input_tokens }}
      max_output_tokens: {{ wintermute_max_output_tokens }}
      max_tokens: {{ wintermute_max_model_len }}
    tpm_limit: 80000
    rpm_limit: 40
    max_concurrent_requests: {{ wintermute_max_concurrent_requests }}

  # ============================================================================
  # LOGICAL ROLE: embeddings-general
  # ============================================================================
  # General-purpose text embeddings
  # Backend: Motoko (BGE-base-en-v1.5, RTX 2080 8GB)
  - model_name: embeddings-general
    litellm_params:
      model: openai/BAAI/bge-base-en-v1.5
      api_base: "{{ motoko_embed_base_url }}"
      api_key: "dummy"
    model_info:
      mode: embedding

  # Legacy alias for backwards compatibility
  - model_name: local/embed
    litellm_params:
      model: openai/BAAI/bge-base-en-v1.5
      api_base: "{{ motoko_embed_base_url }}"
      api_key: "dummy"
    model_info:
      mode: embedding

  # ============================================================================
  # PHYSICAL MODEL ALIASES (for explicit backend selection)
  # ============================================================================
  - model_name: {{ armitage_model_name }}
    litellm_params:
      model: openai/{{ armitage_model_name }}
      api_base: "{{ armitage_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ armitage_max_input_tokens }}
      max_output_tokens: {{ armitage_max_output_tokens }}
      max_tokens: {{ armitage_max_model_len }}

  - model_name: llama31-8b-wintermute
    litellm_params:
      model: {{ wintermute_model_display }}
      api_base: "{{ wintermute_base_url }}"
      api_key: "dummy"
    model_info:
      max_input_tokens: {{ wintermute_max_input_tokens }}
      max_output_tokens: {{ wintermute_max_output_tokens }}
      max_tokens: {{ wintermute_max_model_len }}

  # ============================================================================
  # UPSTREAM FALLBACK PROVIDERS
  # ============================================================================
  # OpenAI strong model (fallback for heavy tasks or when local models fail)
  - model_name: openai/strong
    litellm_params:
      model: "{{ openai_strong_model }}"
      api_key: "${OPENAI_API_KEY}"

  # OpenAI cheap model (backup fallback)
  - model_name: openai/cheap
    litellm_params:
      model: "{{ openai_cheap_model }}"
      api_key: "${OPENAI_API_KEY}"

# ============================================================================
# LITELLM SETTINGS
# ============================================================================
# Note: Authentication disabled for local tailnet access
# All traffic is already secured via Tailscale VPN
general_settings:
  database_url: null  # No database - stateless operation

