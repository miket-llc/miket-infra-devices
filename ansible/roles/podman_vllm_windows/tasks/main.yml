# Copyright (c) 2025 MikeT LLC. All rights reserved.

---
# Deploy vLLM on Windows workstations using Podman Desktop
# This role handles the WSL2 port forwarding complexity

- name: Display vLLM deployment info
  ansible.builtin.debug:
    msg:
      - "═══════════════════════════════════════════════════════════"
      - "vLLM DEPLOYMENT ON {{ inventory_hostname | upper }}"
      - "═══════════════════════════════════════════════════════════"
      - ""
      - "Model: {{ vllm.model }}"
      - "Served Name: {{ vllm.served_model_name }}"
      - "Port: {{ vllm.port }}"
      - ""
      - "═══════════════════════════════════════════════════════════"

# ========================================
# Pre-flight Checks
# ========================================
- name: Verify Podman is available
  win_shell: podman version
  register: podman_version
  changed_when: false
  failed_when: podman_version.rc != 0

- name: Get Podman machine status
  win_shell: podman machine list --format json
  register: podman_machine
  changed_when: false

- name: Verify Podman machine is running
  ansible.builtin.assert:
    that:
      - podman_machine.stdout | from_json | selectattr('Running', 'equalto', true) | list | length > 0
    fail_msg: "Podman machine is not running. Please start it first."

# ========================================
# Get WSL2/Podman Machine IP
# ========================================
- name: Get Podman machine IP address
  win_shell: podman machine ssh ip -4 addr show eth0
  register: podman_ip_output
  changed_when: false

- name: Parse Podman machine IP
  ansible.builtin.set_fact:
    podman_machine_ip: "{{ podman_ip_output.stdout | regex_search('inet ([0-9.]+)', '\\1') | first }}"

- name: Display Podman machine IP
  ansible.builtin.debug:
    msg: "Podman machine IP: {{ podman_machine_ip }}"

# ========================================
# Stop existing container if running
# ========================================
- name: Check for existing vLLM container
  win_shell: "podman ps -a --filter name={{ vllm.container_name }} --format '{{ '{{' }}.Names{{ '}}' }}'"
  register: existing_container
  changed_when: false
  failed_when: false

- name: Stop and remove existing container
  win_shell: "podman rm -f {{ vllm.container_name }}"
  when: existing_container.stdout | trim | length > 0
  register: remove_result
  failed_when: false

# ========================================
# Deploy vLLM Container
# ========================================
- name: Generate vLLM startup script
  win_copy:
    content: |
      # vLLM Startup Script for {{ inventory_hostname }}
      # Generated by Ansible - DO NOT EDIT MANUALLY
      podman run -d --name {{ vllm.container_name }} `
        --device nvidia.com/gpu=all `
        --security-opt label=disable `
        -p 0.0.0.0:{{ vllm.port }}:{{ vllm.port }} `
        --shm-size={{ vllm_shm_size | default('4g') }} `
        {{ vllm.image }} `
        --model {{ vllm.model }} `
        --served-model-name {{ vllm.served_model_name }} `
        --quantization {{ vllm.quantization }} `
        --max-model-len {{ vllm.max_model_len }} `
        --max-num-seqs {{ vllm.max_num_seqs | default(1) }} `
        --gpu-memory-utilization {{ vllm.gpu_memory_utilization }} `
      {% if vllm.kv_cache_dtype is defined %}
        --kv-cache-dtype {{ vllm.kv_cache_dtype }} `
      {% endif %}
        --host 0.0.0.0 `
        --port {{ vllm.port }}
    dest: "C:\\Users\\{{ ansible_user }}\\vllm-start.ps1"

- name: Start vLLM container
  win_shell: "powershell.exe -ExecutionPolicy Bypass -File C:\\Users\\{{ ansible_user }}\\vllm-start.ps1"
  register: container_start
  # Container start returns the container ID, not an error

- name: Verify container started
  win_shell: "podman ps --filter name={{ vllm.container_name }} --format '{{ '{{' }}.Names{{ '}}' }}'"
  register: running_container
  until: running_container.stdout | trim | length > 0
  retries: 3
  delay: 5

# ========================================
# Windows Port Forwarding (WSL2 → Host)
# ========================================
- name: Remove existing port proxy rule
  win_shell: "netsh interface portproxy delete v4tov4 listenport={{ vllm.port }} listenaddress=0.0.0.0"
  failed_when: false
  changed_when: false

- name: Add port proxy rule for external access
  win_shell: "netsh interface portproxy add v4tov4 listenport={{ vllm.port }} listenaddress=0.0.0.0 connectport={{ vllm.port }} connectaddress={{ podman_machine_ip }}"
  register: port_proxy

- name: Verify port proxy rule
  win_shell: "netsh interface portproxy show v4tov4"
  register: proxy_rules
  changed_when: false

- name: Display port proxy configuration
  ansible.builtin.debug:
    msg: "{{ proxy_rules.stdout_lines }}"

# ========================================
# Firewall Configuration
# ========================================
- name: Ensure firewall rule exists for vLLM port
  win_shell: |
    $ruleName = "vLLM Port {{ vllm.port }}"
    $existing = Get-NetFirewallRule -Name $ruleName -ErrorAction SilentlyContinue
    if (-not $existing) {
      New-NetFirewallRule -Name $ruleName -DisplayName $ruleName -Direction Inbound -Action Allow -Protocol TCP -LocalPort {{ vllm.port }}
      Write-Host "Created firewall rule"
    } else {
      Write-Host "Firewall rule already exists"
    }
  register: firewall_result
  changed_when: "'Created' in firewall_result.stdout"

# ========================================
# Wait for model to load
# ========================================
- name: Wait for vLLM to be ready (model loading)
  win_shell: |
    $maxAttempts = 60
    $attempt = 0
    while ($attempt -lt $maxAttempts) {
      try {
        $response = Invoke-WebRequest -Uri "http://localhost:{{ vllm.port }}/health" -UseBasicParsing -TimeoutSec 5
        if ($response.StatusCode -eq 200) {
          Write-Host "vLLM is ready"
          exit 0
        }
      } catch {
        # Still starting up
      }
      Start-Sleep -Seconds 5
      $attempt++
    }
    Write-Host "Timeout waiting for vLLM"
    exit 1
  register: health_wait
  async: 600
  poll: 30

# ========================================
# Final Verification
# ========================================
- name: Verify vLLM responds to models endpoint
  win_shell: "Invoke-WebRequest -Uri 'http://localhost:{{ vllm.port }}/v1/models' -UseBasicParsing | Select-Object -ExpandProperty Content"
  register: models_response
  failed_when: vllm.served_model_name not in models_response.stdout

- name: Display deployment summary
  ansible.builtin.debug:
    msg:
      - "═══════════════════════════════════════════════════════════"
      - "vLLM DEPLOYMENT COMPLETE ON {{ inventory_hostname | upper }}"
      - "═══════════════════════════════════════════════════════════"
      - ""
      - "Container: {{ vllm.container_name }}"
      - "Model: {{ vllm.served_model_name }}"
      - "Local endpoint: http://localhost:{{ vllm.port }}/v1"
      - "Tailnet endpoint: http://{{ inventory_hostname }}.pangolin-vega.ts.net:{{ vllm.port }}/v1"
      - ""
      - "Port proxy: 0.0.0.0:{{ vllm.port }} → {{ podman_machine_ip }}:{{ vllm.port }}"
      - ""
      - "═══════════════════════════════════════════════════════════"

