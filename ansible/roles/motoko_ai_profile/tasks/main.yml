# Copyright (c) 2025 MikeT LLC. All rights reserved.

---
# Motoko AI Profile - GPU-accelerated embeddings + classification
# Uses vLLM for embeddings (TEI requires compute cap 8.0+, RTX 2080 is 7.5)

- name: Display AI Profile deployment info
  ansible.builtin.debug:
    msg:
      - "═══════════════════════════════════════════════════════════"
      - "MOTOKO AI PROFILE DEPLOYMENT"
      - "═══════════════════════════════════════════════════════════"
      - ""
      - "Embedding Model (vLLM GPU):"
      - "  - {{ vllm_embeddings_model }} (port {{ vllm_embeddings_port }})"
      - ""
      - "Classification Model (GPU):"
      - "  - {{ classifier_model }} (port {{ classifier_port }})"
      - ""
      - "GPU Power Limit: {{ gpu_power_limit_watts }}W"
      - "Thermal Thresholds: Normal <{{ gpu_thermal_normal }}°C, Hot <{{ gpu_thermal_hot }}°C"
      - ""
      - "═══════════════════════════════════════════════════════════"
  tags: [ai_profile, always]

# =============================================================================
# Prerequisites
# =============================================================================

- name: Ensure AI profile directories exist
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: '0755'
  loop:
    - "{{ ai_profile_base_dir }}/vllm"
    - "{{ ai_profile_base_dir }}/vllm/cache"
    - "{{ ai_profile_base_dir }}/classifier"
    - "{{ ai_profile_base_dir }}/litellm"
  tags: [ai_profile, setup]

# =============================================================================
# GPU Power Limit
# =============================================================================

- name: Set GPU power limit for thermal protection
  ansible.builtin.command: nvidia-smi -pl {{ gpu_power_limit_watts }}
  register: power_limit_result
  changed_when: "'Power limit set' in power_limit_result.stdout or power_limit_result.rc == 0"
  failed_when: false
  tags: [ai_profile, gpu, thermal]

- name: Create GPU power limit systemd service
  ansible.builtin.copy:
    dest: /etc/systemd/system/gpu-power-limit.service
    content: |
      # Managed by Ansible - motoko_ai_profile role
      [Unit]
      Description=Set GPU Power Limit for Thermal Protection
      After=nvidia-persistenced.service

      [Service]
      Type=oneshot
      ExecStart=/usr/bin/nvidia-smi -pl {{ gpu_power_limit_watts }}
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target
    mode: '0644'
  notify: reload systemd
  tags: [ai_profile, gpu, thermal]

- name: Enable GPU power limit service
  ansible.builtin.systemd:
    name: gpu-power-limit
    enabled: true
    state: started
    daemon_reload: true
  tags: [ai_profile, gpu, thermal]

# =============================================================================
# Thermal Monitor
# =============================================================================

- name: Create GPU thermal monitor script
  ansible.builtin.copy:
    dest: /usr/local/bin/gpu-thermal-monitor
    content: |
      #!/bin/bash
      # Managed by Ansible - motoko_ai_profile role
      # Monitors GPU temperature and writes thermal state to file

      NORMAL_TEMP={{ gpu_thermal_normal }}
      HOT_TEMP={{ gpu_thermal_hot }}
      CRITICAL_TEMP={{ gpu_thermal_critical }}
      STATE_FILE=/tmp/gpu_thermal_state

      while true; do
        TEMP=$(nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits 2>/dev/null || echo "0")
        
        if [ "$TEMP" -ge "$CRITICAL_TEMP" ]; then
          echo "critical" > "$STATE_FILE"
        elif [ "$TEMP" -ge "$HOT_TEMP" ]; then
          echo "hot" > "$STATE_FILE"
        elif [ "$TEMP" -ge "$NORMAL_TEMP" ]; then
          echo "warm" > "$STATE_FILE"
        else
          echo "normal" > "$STATE_FILE"
        fi
        
        # Also write temperature for debugging
        echo "$TEMP" > /tmp/gpu_temperature
        
        sleep 10
      done
    mode: '0755'
  tags: [ai_profile, thermal]

- name: Create thermal monitor systemd service
  ansible.builtin.copy:
    dest: /etc/systemd/system/gpu-thermal-monitor.service
    content: |
      # Managed by Ansible - motoko_ai_profile role
      [Unit]
      Description=GPU Thermal State Monitor
      After=network.target

      [Service]
      Type=simple
      ExecStart=/usr/local/bin/gpu-thermal-monitor
      Restart=always
      RestartSec=5

      [Install]
      WantedBy=multi-user.target
    mode: '0644'
  notify: reload systemd
  tags: [ai_profile, thermal]

- name: Enable thermal monitor service
  ansible.builtin.systemd:
    name: gpu-thermal-monitor
    enabled: true
    state: started
    daemon_reload: true
  tags: [ai_profile, thermal]

# =============================================================================
# vLLM Embeddings (GPU)
# =============================================================================

- name: Create vLLM embeddings compose file
  ansible.builtin.copy:
    dest: "{{ ai_profile_base_dir }}/vllm/docker-compose.yml"
    content: |
      # Managed by Ansible - motoko_ai_profile role
      version: "3.9"

      services:
        vllm-embeddings:
          image: {{ vllm_embeddings_image }}
          container_name: {{ vllm_embeddings_container_name }}
          command: >
            --model {{ vllm_embeddings_model }}
            --host 0.0.0.0
            --port 8000
            --gpu-memory-utilization {{ vllm_embeddings_gpu_util }}
            --tensor-parallel-size 1
            --trust-remote-code
          ports:
            - "{{ vllm_embeddings_port }}:8000"
          volumes:
            - {{ ai_profile_base_dir }}/vllm/cache:/root/.cache/huggingface:Z
          environment:
            - HF_HOME=/root/.cache/huggingface
            - NVIDIA_VISIBLE_DEVICES=all
          devices:
            - nvidia.com/gpu=all
          security_opt:
            - label:disable
          restart: unless-stopped
          healthcheck:
            test: ["CMD", "curl", "-fsS", "http://127.0.0.1:8000/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 120s
    mode: '0644'
  when: vllm_embeddings_enabled
  tags: [ai_profile, embeddings]

- name: Start vLLM embeddings container
  ansible.builtin.shell: |
    cd {{ ai_profile_base_dir }}/vllm && podman-compose down 2>/dev/null || true
    cd {{ ai_profile_base_dir }}/vllm && podman-compose up -d
  environment:
    TMPDIR: /podman/tmp
  when: vllm_embeddings_enabled
  tags: [ai_profile, embeddings]

# =============================================================================
# Classification Service (GPU)
# =============================================================================

- name: Create classifier Python application
  ansible.builtin.template:
    src: classifier_app.py.j2
    dest: "{{ ai_profile_base_dir }}/classifier/app.py"
    mode: '0644'
  when: classifier_enabled
  tags: [ai_profile, classifier]

- name: Create classifier requirements.txt
  ansible.builtin.copy:
    dest: "{{ ai_profile_base_dir }}/classifier/requirements.txt"
    content: |
      fastapi>=0.104.0
      uvicorn[standard]>=0.24.0
      transformers>=4.35.0
      torch>=2.1.0
      accelerate>=0.24.0
      sentencepiece>=0.1.99
    mode: '0644'
  when: classifier_enabled
  tags: [ai_profile, classifier]

- name: Create classifier Dockerfile
  ansible.builtin.copy:
    dest: "{{ ai_profile_base_dir }}/classifier/Dockerfile"
    content: |
      FROM python:3.11-slim
      
      WORKDIR /app
      
      # Install system dependencies
      RUN apt-get update && apt-get install -y --no-install-recommends \
          curl \
          && rm -rf /var/lib/apt/lists/*
      
      # Install Python dependencies
      COPY requirements.txt .
      RUN pip install --no-cache-dir -r requirements.txt
      
      # Model downloaded at runtime (avoids HF rate limits during build)
      COPY app.py .
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
    mode: '0644'
  when: classifier_enabled
  tags: [ai_profile, classifier]

- name: Build classifier container image
  ansible.builtin.command: >
    podman build -t classifier-motoko:latest {{ ai_profile_base_dir }}/classifier
  environment:
    TMPDIR: /podman/tmp
  register: build_classifier
  changed_when: "'Successfully tagged' in build_classifier.stdout or build_classifier.rc == 0"
  when: classifier_enabled
  tags: [ai_profile, classifier]

- name: Create classifier compose file
  ansible.builtin.template:
    src: classifier-compose.yml.j2
    dest: "{{ ai_profile_base_dir }}/classifier/docker-compose.yml"
    mode: '0644'
  when: classifier_enabled
  tags: [ai_profile, classifier]

- name: Start classifier container
  ansible.builtin.shell: |
    cd {{ ai_profile_base_dir }}/classifier && podman-compose down 2>/dev/null || true
    cd {{ ai_profile_base_dir }}/classifier && podman-compose up -d
  environment:
    TMPDIR: /podman/tmp
  when: classifier_enabled
  tags: [ai_profile, classifier]

# =============================================================================
# LiteLLM Configuration
# =============================================================================

- name: Ensure LiteLLM directory exists
  ansible.builtin.file:
    path: "{{ ai_profile_base_dir }}/litellm"
    state: directory
    mode: '0755'
  tags: [ai_profile, litellm]

- name: Create LiteLLM config for AI profile
  ansible.builtin.copy:
    dest: "{{ ai_profile_base_dir }}/litellm/config.yaml"
    content: |
      # Managed by Ansible - motoko_ai_profile role
      # GPU-accelerated embeddings + classification

      model_list:
        # Embeddings (vLLM on GPU)
        - model_name: {{ litellm_embed_bge_name }}
          litellm_params:
            model: openai/{{ vllm_embeddings_model }}
            api_base: http://127.0.0.1:{{ vllm_embeddings_port }}/v1
            api_key: none
          model_info:
            description: "GPU embeddings - 768 dimensions, 512 token max"

        # OpenAI compatibility alias
        - model_name: text-embedding-ada-002
          litellm_params:
            model: openai/{{ vllm_embeddings_model }}
            api_base: http://127.0.0.1:{{ vllm_embeddings_port }}/v1
            api_key: none

        # Classification (GPU)
        - model_name: {{ litellm_classify_name }}
          litellm_params:
            model: huggingface/{{ classifier_model }}
            api_base: http://127.0.0.1:{{ classifier_port }}
            api_key: none
          model_info:
            description: "Zero-shot classification - 100+ languages"

      litellm_settings:
        drop_params: true
        set_verbose: false
        request_timeout: 60

      general_settings:
        master_key: {{ litellm_master_key }}
    mode: '0644'
  tags: [ai_profile, litellm]

- name: Restart LiteLLM with new config
  ansible.builtin.shell: |
    podman stop litellm-motoko 2>/dev/null || true
    podman rm litellm-motoko 2>/dev/null || true
    podman run -d \
      --name litellm-motoko \
      --network host \
      -v {{ ai_profile_base_dir }}/litellm/config.yaml:/app/config.yaml:ro,Z \
      -e LITELLM_MASTER_KEY={{ litellm_master_key }} \
      --restart unless-stopped \
      ghcr.io/berriai/litellm:main-latest \
      --config /app/config.yaml --port {{ litellm_port }}
  tags: [ai_profile, litellm]

# =============================================================================
# Summary
# =============================================================================

- name: Wait for services to start
  ansible.builtin.pause:
    seconds: 30
  tags: [ai_profile, verify]

- name: Verify services are running
  ansible.builtin.shell: |
    echo "Container Status:"
    podman ps --format "table {% raw %}{{.Names}}\t{{.Status}}{% endraw %}"
    echo ""
    echo "GPU Status:"
    nvidia-smi --query-gpu=temperature.gpu,power.draw,memory.used --format=csv
  register: verify_result
  changed_when: false
  tags: [ai_profile, verify]

- name: Display deployment summary
  ansible.builtin.debug:
    msg:
      - "═══════════════════════════════════════════════════════════"
      - "MOTOKO AI PROFILE DEPLOYMENT COMPLETE"
      - "═══════════════════════════════════════════════════════════"
      - ""
      - "{{ verify_result.stdout_lines | default(['Status check pending...']) }}"
      - ""
      - "Endpoints:"
      - "  Embeddings (vLLM):  http://127.0.0.1:{{ vllm_embeddings_port }}/v1/embeddings"
      - "  Classification:     http://127.0.0.1:{{ classifier_port }}/classify"
      - "  LiteLLM Proxy:      http://127.0.0.1:{{ litellm_port }}/v1/..."
      - ""
      - "LiteLLM Models:"
      - "  - {{ litellm_embed_bge_name }}"
      - "  - {{ litellm_classify_name }}"
      - "  - text-embedding-ada-002 (alias)"
      - ""
      - "═══════════════════════════════════════════════════════════"
  tags: [ai_profile, always]
