# Wintermute - Windows Development/Gaming Workstation
# Last Updated: 2025-11-08

device:
  hostname: wintermute
  type: workstation
  location: office
  primary_user: mdt

hardware:
  cpu: "Intel Core i9 (placeholder)"
  gpu: "NVIDIA GeForce RTX 4070 Super"
  gpu_vram_gb: 12
  memory_gb: 32

operating_system:
  os: Windows 11 Pro
  version: 22H2
  wsl: true
  docker_desktop: true

network:
  tailscale_enabled: true
  remote_desktop: true
  wake_on_lan: true

services:
  - Docker Desktop with WSL2 backend
  - NVIDIA CUDA Toolkit
  - WSL2 (Ubuntu 22.04)
  - Development IDEs (VS Code, Visual Studio)
  - NVIDIA Container Toolkit (via WSL2)

use_cases:
  - Desktop development workstation
  - High-performance gaming
  - CUDA/ML development
  - Docker containerization
  - Remote access via Tailscale

vllm:
  enabled: true
  model: "Qwen/Qwen2.5-7B-Instruct-AWQ"
  port: 8000
  container_name: "vllm-wintermute"
  image: "vllm/vllm-openai:latest"
  gpu_memory_utilization: 0.90
  max_model_len: 8192
  auto_switch: true
  check_interval_minutes: 5
  idle_threshold_minutes: 5
  # For 12GB VRAM, recommended models:
  # - Qwen/Qwen2.5-7B-Instruct-AWQ (~5GB VRAM) - BEST CHOICE: Most capable model that fits comfortably
  #   Optimized settings: 0.90 util, 8192 max_len (allows long context)
  #   Benchmarks: MMLU 74.2, MATH 75.5, HumanEval 84.8, GSM8K 91.6
  # - Qwen/Qwen2.5-14B-Instruct-AWQ (~9GB VRAM) - Too large, insufficient KV cache memory
  # - Qwen/Qwen2.5-32B-Instruct-AWQ (~12GB VRAM) - Maximum capability (does not fit)

notes:
  - "Primary Windows development and gaming workstation"
  - "12GB VRAM optimized for larger models"
  - "vLLM serving via WSL2 Docker"
