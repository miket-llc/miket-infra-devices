# Armitage - Windows Gaming/Development Laptop
# Last Updated: 2024-11-07

device:
  hostname: armitage
  type: laptop
  location: mobile
  primary_user: mdt

hardware:
  manufacturer: Alienware
  model: Gaming Laptop
  cpu: Intel Core i9-13900HX (24 cores, 32 threads)
  gpu: NVIDIA GeForce RTX 4070 (8GB VRAM)
  memory: 32GB DDR5
  storage:
    - 2TB NVMe SSD (Primary)
    - 1TB NVMe SSD (Games/Data)

operating_system:
  os: Windows 11 Pro
  version: 22H2
  wsl: true
  docker_desktop: true

network:
  tailscale_enabled: true
  remote_desktop: true
  wake_on_lan: true

services:
  - Docker Desktop with WSL2 backend
  - NVIDIA CUDA Toolkit
  - WSL2 (Ubuntu 22.04)
  - Steam/Gaming platforms
  - Development IDEs (VS Code, Visual Studio)
  - NVIDIA Container Toolkit (via WSL2)

use_cases:
  - Mobile development workstation
  - High-performance gaming
  - CUDA/ML development
  - Docker containerization
  - Remote access via Tailscale

vllm:
  enabled: true
  model: "Qwen/Qwen2.5-7B-Instruct-AWQ"
  port: 8000
  container_name: "vllm-armitage"
  image: "vllm/vllm-openai:latest"
  quantization: "awq"
  max_model_len: 16384
  max_num_seqs: 1
  gpu_memory_utilization: 0.85
  served_model_name: "qwen2.5-7b-armitage"
  tensor_parallel_size: 1
  auto_switch: true
  check_interval_minutes: 5
  idle_threshold_minutes: 5
  # For 8GB VRAM, fp16/bf16 build (current configuration):
  # - Qwen/Qwen2.5-7B-Instruct (bf16) - fp16/bf16 build as specified
  #   Settings: 0.9 util, 8192 max_len, 2 max_seqs (optimized for fp16/bf16)
  #   Note: AWQ quantized version available if memory constraints require it
  #   AWQ version: Qwen/Qwen2.5-7B-Instruct-AWQ (~5GB VRAM, use kv_cache_dtype: fp8)

notes:
  - High-performance Alienware laptop with RTX 4070
  - Optimized for both gaming and development workflows
  - Supports GPU passthrough to WSL2 for CUDA workloads
  - Connected to Tailnet for secure remote access
  - Gaming mode optimizations available via Windows Game Mode
  - vLLM serving with automatic mode switching between workstation and LLM modes
