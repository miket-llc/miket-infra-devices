# Copyright (c) 2025 MikeT LLC. All rights reserved.
#
# Armitage - Fedora KDE Workstation + Ollama LLM Node
# Last Updated: 2025-12-20
#
# Architecture References:
#   - ADR-004-kde-plasma-linux-ui-standard.md: KDE Plasma is the standard desktop
#   - ADR-005-llm-runtime-llama-vs-vllm.md: Workstations use Ollama pattern
#   - AI_FABRIC_SERVICE.md: Dual-pattern LLM architecture
#   - AI_FABRIC_PLATFORM_CONTRACT.md: Node topology and ports

device:
  hostname: armitage
  type: laptop
  location: mobile
  primary_user: mdt

  # Device classification
  roles:
    - linux_workstation
    - gpu_node
    - llm_node
    - cuda_development
    - kde_desktop
    - gaming

hardware:
  manufacturer: Alienware
  model: m16 R2
  cpu: Intel Core i9-13900HX (24 cores, 32 threads)
  gpu: NVIDIA GeForce RTX 4070 Max-Q (8GB VRAM, CUDA capable)
  gpu_driver: "580.119.02"
  memory: 16GB DDR5
  storage:
    - 2TB NVMe SSD (Primary - Fedora + data)

operating_system:
  os: Fedora
  version: "43"
  desktop: KDE Plasma       # ADR-004: KDE is the Linux UI standard
  selinux: enforcing        # Proper SELinux policy for Tailscale SSH
  container_runtime: podman
  docker_desktop: false     # Docker is BANNED - Podman only

network:
  tailscale_enabled: true
  tailscale_ip: 100.88.153.51
  tailscale_hostname: armitage.pangolin-vega.ts.net
  tailscale_tags:
    - linux
    - gaming
    - workstation
  tailscale_ssh: true
  remote_desktop: nomachine
  wake_on_lan: true

services:
  # LLM Runtime (ADR-005: Ollama pattern for workstations)
  - Ollama (workstation LLM runtime)
  # Container Runtime
  - Podman (container runtime)
  # GPU/Development
  - NVIDIA CUDA Toolkit
  - NVIDIA Container Toolkit
  # Development IDEs
  - VS Code (Flatpak)
  - Warp Terminal (Flatpak)
  # Remote Access
  - NoMachine (remote desktop)

# =============================================================================
# LLM Configuration: Ollama Workstation Pattern
# =============================================================================
# Per ADR-005 and AI Fabric Platform Contract:
#   - armitage runs Ollama (workstation pattern)
#   - akira runs vLLM (server pattern)
#
# Ports per AI Fabric ACLs:
#   - 11434: Ollama API (default)
#   - 8000: LLM Gateway (optional, for LiteLLM proxy)
ollama:
  enabled: true
  port: 11434
  api_endpoint: "http://armitage.pangolin-vega.ts.net:11434"
  models:
    - qwen2.5:7b
    - llama3.2:3b
  # Filesystem layout (Flux/Space/Time standard)
  config_path: /flux/apps/ollama
  models_path: /space/llm/ollama/models
  data_path: /space/llm/ollama/data

# Optional LLM Gateway on port 8000 (per AI Fabric contract)
llm_gateway:
  enabled: false
  port: 8000
  type: litellm  # or custom proxy

use_cases:
  - Local LLM inference via Ollama
  - Mobile development workstation
  - CUDA/ML development
  - Podman containerization
  - Remote access via Tailscale

# =============================================================================
# IMPORTANT: Windows Partition (Out-of-Band)
# =============================================================================
# A small Windows partition exists for Dell support/diagnostics ONLY.
#
# This partition is:
#   ❌ NOT joined to tailnet
#   ❌ NOT managed by Ansible
#   ❌ NOT part of any infrastructure automation
#   ❌ NOT used for regular workloads
#
# It is a recovery/diagnostic partition only. All active work uses Fedora KDE.
windows_partition:
  purpose: "Dell support, diagnostics, BIOS updates"
  on_tailnet: false
  managed_by_ansible: false
  note: "Out-of-band utility partition. Boot into Windows ONLY for hardware diagnostics."

notes:
  - High-performance Alienware m16 R2 laptop with RTX 4070 Max-Q
  - Rebuilt from Fedora 43 GNOME to Fedora 43 KDE (2025-12-20)
  - KDE Plasma desktop per ADR-004
  - Ollama LLM runtime per ADR-005 (not vLLM)
  - Connected to Tailnet with linux/gaming/workstation tags
  - LLM ports 11434/8000 accessible via tailnet ACLs
  - SELinux enforcing with custom policy for Tailscale SSH
  - NVIDIA driver 580.119.02 via RPM Fusion akmod
