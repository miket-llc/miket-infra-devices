# LiteLLM Proxy Configuration
# Location: /etc/litellm/config.yaml (or ~/litellm/config.yaml)
# This file configures LiteLLM to proxy requests to vLLM instances
# Tailnet: tail2e55fe.ts.net

model_list:
  # Armitage - Qwen2.5-7B-Instruct (8GB VRAM)
  - model_name: qwen2.5-7b-armitage
    litellm_params:
      model: openai/Qwen-Qwen2.5-7B-Instruct
      api_base: http://armitage.tail2e55fe.ts.net:8000/v1
      api_key: dummy  # vLLM doesn't require auth, but LiteLLM needs a key
    model_info:
      mode: "completion"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_input_tokens: 7000
      max_output_tokens: 768
      supports_function_calling: false
      supports_vision: false
    tpm: 80000
    rpm: 40
    max_parallel_requests: 2

  # Wintermute - Llama-3.1-8B-Instruct (AWQ) - Reference node
  - model_name: llama-3.1-8b-wintermute
    litellm_params:
      model: openai/meta-llama/Llama-3.1-8B-Instruct-AWQ
      api_base: http://wintermute.tail2e55fe.ts.net:8000/v1
      api_key: dummy
    model_info:
      mode: "completion"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_input_tokens: 8000
      max_output_tokens: 1024
      supports_function_calling: false
      supports_vision: false
    tpm: 100000
    rpm: 50
    max_parallel_requests: 3

# General settings
general_settings:
  master_key: "your-master-key-here"  # Change this in production
  database_url: "sqlite:///litellm.db"
  success_callback: []
  failure_callback: []

# Server settings
server_settings:
  host: "0.0.0.0"
  port: 4000
  num_workers: 1
  timeout: 300

# Logging
litellm_settings:
  set_verbose: true
  suppress_debug_logs: false

